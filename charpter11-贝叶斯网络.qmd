# 贝叶斯网络

```{r}
#| warning: false
#| echo: false
source('scripts/utils.R')
library(bnlearn)
library(Rgraphviz)
library(dbnR)
library(gRain)
library(janitor)
```

在现实世界中，变量之间往往存在复杂的依赖关系和因果关系。传统的机器学习方法虽然能够处理这些关系，但往往缺乏对不确定性的建模能力和对因果关系的明确表示。贝叶斯网络作为概率图模型的重要分支，为解决这一问题提供了优雅的解决方案。

贝叶斯网络巧妙地将图论、概率论和统计学结合在一起，不仅能够表示变量间的依赖关系，还能够处理不确定性推理问题。经过几十年的发展，贝叶斯网络已经成为人工智能、机器学习、统计学等领域的重要工具。本章将系统地介绍贝叶斯网络的基本概念、理论基础，以及从数据中学习贝叶斯网络的两个核心问题：结构学习和参数学习。

## 贝叶斯网络基础理论

### 基本定义与概念

贝叶斯网络（Bayesian Network），也称为信念网络（Belief Network）或因果网络（Causal Network），是一个有向无环图（Directed Acyclic Graph, DAG），用于表示一组随机变量及其条件依赖关系。形式化地，贝叶斯网络由两个部分组成：

1. **图结构**：一个有向无环图G = (V, E)，其中V是节点集合，每个节点代表一个随机变量；E是边集合，有向边表示变量间的直接依赖关系。

2. **参数集合**：每个节点Xi都关联一个条件概率分布P(Xi | Pa(Xi))，其中Pa(Xi)表示节点Xi的父节点集合。

贝叶斯网络的核心思想是通过图结构来编码变量间的条件独立关系，从而实现联合概率分布的紧凑表示。对于n个随机变量X₁, X₂, ..., Xₙ，贝叶斯网络将联合概率分布分解为：

$$P(X_1, X_2, \ldots, X_n) = \prod_{i=1}^{n} P(X_i | Pa(X_i))$$

这种分解被称为**链式法则的图形化表示**，它减少了需要存储的参数数量。例如，对于n个二值变量，完整的联合概率分布需要2ⁿ-1个参数，而贝叶斯网络只需要∑ᵢ2^|Pa(Xᵢ)|个参数，其中|Pa(Xᵢ)|是节点Xᵢ的父节点数量。

### 条件独立性与d-分离

贝叶斯网络的一个重要特性是它能够隐式地编码变量间的条件独立关系。理解这些独立关系对于网络的构建、推理和学习都至关重要。

**局部马尔可夫性质**是贝叶斯网络的基础性质：给定其父节点，每个节点与其所有非后代节点条件独立。形式化表示为：

$$X_i \perp NonDescendants(X_i) | Pa(X_i)$$

**d-分离**（d-separation）是判断贝叶斯网络中任意两个节点集合是否条件独立的图形化准则。Pearl提出的d-分离算法考虑了三种基本的连接模式：

1. **串联连接**（Serial Connection）：A → B → C
   - 如果B未被观测，则A和C不独立
   - 如果B被观测，则A和C条件独立

2. **发散连接**（Diverging Connection）：A ← B → C
   - 如果B未被观测，则A和C不独立
   - 如果B被观测，则A和C条件独立

3. **汇聚连接**（Converging Connection）：A → B ← C
   - 如果B及其后代都未被观测，则A和C独立
   - 如果B或其任一后代被观测，则A和C不独立

### 贝叶斯网络的表示能力

**因果关系建模**：虽然贝叶斯网络本质上是概率模型，但其有向图结构天然地暗示了因果关系。在Pearl的因果推理框架中，贝叶斯网络被扩展为因果图，能够处理干预和反事实推理。

**不确定性量化**：贝叶斯网络通过概率分布来量化不确定性，能够处理噪声数据和不完整信息。

**知识整合**：贝叶斯网络能够将专家知识和数据驱动的学习相结合，既可以从先验知识出发构建网络，也可以从数据中学习网络结构。

## 结构学习：从数据中发现依赖关系

结构学习是贝叶斯网络学习的核心问题之一，其目标是从观测数据中学习变量间的依赖关系，确定网络的拓扑结构。这个问题具有重要的理论意义和实际价值，因为它直接关系到我们对数据生成机制的理解。

### 结构学习的理论基础

结构学习主要面临：

**搜索空间的巨大性**：n个节点的有向无环图数量呈现超指数增长。对于n个节点，可能的DAG数量由下式给出：

$$|DAG_n| = \sum_{i=1}^{n} (-1)^{n-i} \binom{n}{i} 2^{i(i-1)/2} |DAG_i|$$

这个数量增长极快，使得穷举搜索在实际问题中不可行。

**等价类问题**：多个不同的DAG可能编码相同的条件独立关系，形成马尔可夫等价类。在没有干预数据的情况下，仅从观测数据无法区分等价类中的不同结构。

**有限样本效应**：实际应用中的数据通常是有限的，这导致统计检验的可靠性下降，进而影响结构学习的准确性。

### 基于约束的结构学习方法

基于约束的方法通过统计检验来发现变量间的条件独立关系，然后利用这些关系来构建网络结构。

#### PC算法

PC算法（Peter-Clark算法）是最具代表性的基于约束的算法，由Spirtes等人提出。算法的基本思想是：

1. **骨架构建阶段**：
   - 从完全无向图开始
   - 对每对变量进行条件独立检验
   - 移除条件独立的变量间的边

2. **边方向确定阶段**：
   - 识别v-结构（A → B ← C，其中A和C不相邻）
   - 应用方向传播规则，避免形成有向环

PC算法的理论保证是：在大样本极限下，如果条件独立检验是准确的，算法能够准确恢复真实网络的马尔可夫等价类。

**算法的详细步骤**：

*第一阶段：骨架学习*
```{}
输入：变量集合V，数据集D，显著性水平α
输出：无向图G

1. 初始化G为完全无向图
2. l = 0
3. repeat:
   4. 对G中每条边(X,Y)：
      5. 如果|Adj(G,X)\{Y}| ≥ l：
         6. 对Adj(G,X)\{Y}中所有大小为l的子集S：
            7. 如果X ⊥ Y | S：
               8. 从G中移除边(X,Y)
               9. 记录分离集Sep(X,Y) = S
   10. l = l + 1
11. until 不能再移除边
```

*第二阶段：边方向确定*
```{}
1. 对每个三元组(X,Y,Z)，如果X-Y-Z且X和Z不相邻：
   2. 如果Y ∉ Sep(X,Z)：
      3. 将边方向确定为X → Y ← Z

4. repeat:
   5. 应用以下规则直到不能再确定方向：
      - R1：如果X → Y - Z且X和Z不相邻，则Y → Z
      - R2：如果X → Y → Z且X - Z，则X → Z
      - R3：如果X - Y - Z，X → W → Z，Y → W，则Y → Z
6. until 不能再确定边方向
```

####  IC算法及其变种

IC算法（Inductive Causation）是PC算法的改进版本，主要改进包括：

- 更高效的条件独立检验策略
- 改进的边方向确定规则
- 更好的统计功效

**FCI算法**（Fast Causal Inference）扩展了IC算法，能够处理隐变量的情况。FCI算法不仅能够发现观测变量间的直接因果关系，还能够识别可能的混杂因子。

#### 条件独立检验

基于约束方法的成功很大程度上依赖于条件独立检验的准确性。常用的检验方法包括：

**卡方检验**：适用于离散变量
$$\chi^2 = \sum_{i,j,k} \frac{(O_{ijk} - E_{ijk})^2}{E_{ijk}}$$

其中$O_{ijk}$是观测频数，$E_{ijk}$是期望频数。

**偏相关检验**：适用于连续变量
$$r_{XY|Z} = \frac{r_{XY} - r_{XZ}r_{YZ}}{\sqrt{(1-r_{XZ}^2)(1-r_{YZ}^2)}}$$

**互信息检验**：适用于混合类型变量
$$I(X;Y|Z) = \sum_{x,y,z} P(x,y,z) \log \frac{P(x,y|z)}{P(x|z)P(y|z)}$$

###  基于评分的结构学习方法

基于评分的方法将结构学习转化为优化问题：定义一个评分函数来衡量网络结构与数据的拟合程度，然后搜索使评分函数最优的网络结构。

####  评分函数设计

一个好的评分函数应该平衡模型的拟合能力和复杂度。常用的评分函数包括：

**贝叶斯信息准则（BIC）**：
$$BIC(G,D) = LL(G,D) - \frac{d}{2}\log N$$

LL(G,D)$是给定结构G和数据D的对数似然$d$是网络参数的个数,$N$是样本数量。

BIC评分在大样本条件下能够一致地识别真实模型结构。

**赤池信息准则（AIC）**：
$$AIC(G,D) = LL(G,D) - d$$

AIC相比BIC对模型复杂度的惩罚较轻，在小样本情况下可能表现更好。

**贝叶斯-狄利克雷评分（BD评分）**：
$$BD(G,D) = \log P(G) + \sum_{i=1}^{n} \sum_{j=1}^{q_i} \log \frac{\Gamma(\alpha_{ij})}{\Gamma(\alpha_{ij} + N_{ij})} \sum_{k=1}^{r_i} \log \frac{\Gamma(\alpha_{ijk} + N_{ijk})}{\Gamma(\alpha_{ijk})}$$

其中：
- $P(G)$是结构的先验概率
- $\alpha_{ijk}$是先验参数
- $N_{ijk}$是对应的统计量
- $q_i$是节点$i$的父节点配置数
- $r_i$是节点$i$的状态数

BD评分在贝叶斯框架下具有理论最优性，但计算复杂度较高。

**K2评分**：
$$K2(G,D) = \prod_{i=1}^{n} \prod_{j=1}^{q_i} \frac{(r_i-1)!}{(N_{ij}+r_i-1)!} \prod_{k=1}^{r_i} N_{ijk}!$$

K2评分是BD评分在均匀先验条件下的特殊情况，计算相对简单。

#### 结构搜索算法

由于结构搜索空间的巨大性，需要高效的搜索算法。

**贪心搜索算法**：

贪心搜索是最常用的局部搜索方法，基本思想是从某个初始结构开始，每次选择能够最大程度改善评分函数的结构变化操作。

常见的结构变化操作包括：
- **边添加**：在两个不相邻的节点间添加有向边
- **边删除**：删除现有的有向边
- **边反转**：改变有向边的方向

算法流程：
```{}
输入：数据集D，初始结构G₀，评分函数Score
输出：学习到的网络结构G*

1. G = G₀
2. repeat:
3.   best_score = Score(G,D)
4.   best_operation = null
5.   for 每个可能的结构操作op:
6.     G' = apply(op, G)
7.     if G'是有向无环图 and Score(G',D) > best_score:
8.       best_score = Score(G',D)
9.       best_operation = op
10.  if best_operation ≠ null:
11.    G = apply(best_operation, G)
12. until best_operation = null
13. return G
```

**K2算法**：

K2算法假设变量有预定义的拓扑排序，这大大简化了搜索过程。对于每个节点，K2算法贪心地选择其父节点集合。

```{}
输入：变量排序π，数据集D，最大父节点数u
输出：网络结构G

1. for i = 1 to n:
2.   Pa[i] = ∅
3.   P_old = Score(i, Pa[i], D)
4.   OKToProceed = true
5.   while OKToProceed and |Pa[i]| < u:
6.     让z为π中在i之前且不在Pa[i]中使Score(i, Pa[i]∪{z}, D)最大的节点
7.     P_new = Score(i, Pa[i]∪{z}, D)
8.     if P_new > P_old:
9.       P_old = P_new
10.      Pa[i] = Pa[i]∪{z}
11.    else:
12.      OKToProceed = false
13. 构造网络G使每个节点i的父节点为Pa[i]
14. return G
```

**遗传算法和模拟退火**：

对于复杂的搜索空间，可以采用全局优化方法：

- **遗传算法**：通过选择、交叉和变异操作来进化网络结构群体
- **模拟退火**：通过逐渐降低"温度"来跳出局部最优
- **粒子群优化**：模拟鸟群觅食行为的优化算法

#### 搜索空间约束

为了提高搜索效率，常常需要对搜索空间进行约束：

**变量排序约束**：预先定义变量的部分或完全排序，减少需要考虑的边方向。

**最大父节点数约束**：限制每个节点的最大父节点数，控制网络的复杂度。

**先验知识约束**：利用专家知识禁止或强制某些边的存在。

**分层结构约束**：将变量分为不同层次，只允许从上层到下层的连接。

### 混合方法

为了结合约束方法和评分方法的优势，研究者提出了多种混合方法。

#### MMHC算法

最大最小爬山（Max-Min Hill Climbing）算法分两个阶段：

1. **约束阶段**：使用最大最小父子算法（MMPC）为每个变量识别候选父子节点
2. **评分阶段**：在约束的搜索空间内使用爬山搜索找到最优结构

MMHC算法的优势在于：
- 约束阶段大大减少了搜索空间
- 评分阶段保证了结构的质量
- 对高维数据具有良好的扩展性

#### RSL算法

限制搜索列表（Restricted Search List）算法通过以下步骤工作：

1. 使用快速的约束方法生成每个变量的候选父节点列表
2. 在这些约束下进行基于评分的搜索
3. 迭代优化直到收敛

#### 三阶段算法

一些研究提出了三阶段的学习框架：

1. **草图学习**：使用快速算法学习网络的大致结构
2. **结构细化**：在草图基础上进行局部搜索优化
3. **参数学习**：学习最终结构的参数

###  特殊情况下的结构学习

####  高维数据的结构学习

当变量数量远大于样本数量时，传统方法面临严重挑战。针对高维情况的方法包括：

**正则化方法**：
- L1正则化促进稀疏结构
- 结构化正则化考虑网络的特殊性质

**分而治之策略**：
- 将高维问题分解为多个低维子问题
- 使用聚类方法预处理变量

**近似推理方法**：
- 变分推理近似复杂的后验分布
- 采样方法处理不确定性

####  时序数据的结构学习

动态贝叶斯网络（DBN）扩展了静态贝叶斯网络，能够建模时序依赖关系：

$$P(X_1^{(t)}, \ldots, X_n^{(t)} | X_1^{(t-1)}, \ldots, X_n^{(t-1)})$$

时序结构学习需要考虑：
- 时间滞后关系
- 结构的时变性
- 计算效率

####  缺失数据下的结构学习

实际数据常常存在缺失值，这对结构学习提出了挑战：

**完全案例分析**：只使用完整的样本，可能导致信息损失和偏差。

**EM算法**：迭代估计缺失值和网络结构。

**多重插补**：生成多个完整数据集，分别学习结构后综合结果。

**基于似然的方法**：直接在有缺失数据的似然函数上进行优化。

##  参数学习：估计条件概率分布

参数学习的目标是在给定网络结构的情况下，从数据中估计每个节点的条件概率分布。这个问题相对于结构学习来说更加成熟，有着坚实的统计理论基础。

###  参数学习的理论框架

参数学习可以在两个不同的统计框架下进行：

**频率主义框架**：将参数视为固定但未知的常数，通过数据来估计参数的点值。

**贝叶斯框架**：将参数视为随机变量，通过先验分布和数据来计算参数的后验分布。

两种框架各有优势：频率主义方法计算简单，解释直观；贝叶斯方法能够量化参数不确定性，更好地处理小样本问题。

###  完整数据的参数学习

当观测数据完整（无缺失值）时，参数学习相对简单。

####  最大似然估计

最大似然估计（MLE）是最常用的参数估计方法。对于给定的网络结构G和数据集D = {x₁, x₂, ..., xₘ}，似然函数为：

$$L(\theta | G, D) = \prod_{l=1}^{m} \prod_{i=1}^{n} P(x_i^{(l)} | pa_i^{(l)}, \theta_i)$$

其中$\theta_i$是节点i的参数，$x_i^{(l)}$是第l个样本中节点i的值，$pa_i^{(l)}$是相应的父节点值。

对于离散变量，MLE的解析解为：

$$\hat{\theta}_{ijk} = \frac{N_{ijk}}{N_{ij}}$$

其中：
- $N_{ijk}$是变量$X_i$取第k个值且其父节点取第j个配置的样本数
- $N_{ij} = \sum_{k=1}^{r_i} N_{ijk}$是父节点取第j个配置的总样本数

对于连续变量（假设高斯分布），参数估计为：

$$\hat{\mu}_{i|pa_i} = \frac{1}{N_{pa_i}} \sum_{l: pa_i^{(l)} = pa_i} x_i^{(l)}$$

$$\hat{\sigma}_{i|pa_i}^2 = \frac{1}{N_{pa_i}} \sum_{l: pa_i^{(l)} = pa_i} (x_i^{(l)} - \hat{\mu}_{i|pa_i})^2$$

####  贝叶斯估计

贝叶斯估计通过引入先验分布来处理参数不确定性。

**狄利克雷先验**：

对于离散变量的多项分布参数，狄利克雷分布是共轭先验：

$$P(\theta_{ij} | \alpha_{ij}) = Dir(\alpha_{ij1}, \ldots, \alpha_{ijr_i})$$

其中$\alpha_{ijk} > 0$是先验参数。

贝叶斯估计的结果为：

$$\hat{\theta}_{ijk} = \frac{N_{ijk} + \alpha_{ijk}}{N_{ij} + \alpha_{ij}}$$

其中$\alpha_{ij} = \sum_{k=1}^{r_i} \alpha_{ijk}$。

**正态-逆伽马先验**：

对于连续变量的高斯分布参数，正态-逆伽马分布是共轭先验：

$$P(\mu, \sigma^2) = \mathcal{N}(\mu | \mu_0, \frac{\sigma^2}{\lambda_0}) \cdot \text{IG}(\sigma^2 | \alpha_0, \beta_0)$$

**先验参数的选择**：

- **无信息先验**：$\alpha_{ijk} = 1$（Laplace平滑）
- **专家知识先验**：根据领域专家的判断设定
- **经验贝叶斯**：从数据中估计先验参数

####  参数的不确定性量化

贝叶斯方法的一个重要优势是能够量化参数估计的不确定性：

$$P(\theta | D, G) = \frac{P(D | \theta, G) P(\theta | G)}{P(D | G)}$$

这个后验分布提供了参数不确定性的完整描述，可用于：
- 计算CI
- 进行预测时考虑参数不确定性
- 模型选择和比较

## 数据
冠状动脉血栓形成风险数据来源于1841名男性的数据。
```{r}
data(coronary,package = 'bnlearn')
coronary <- coronary %>% clean_names()

var_desc <- data.frame(
  变量名 = names(coronary),
  描述 = c(
    "吸烟（smoking）：二分类因子",
    "脑力劳动（M. Work，高强度脑力工作）：二分类因子",
    "体力劳动（P. Work，高强度体力工作）：二分类因子",
    "血压（Pressure，收缩压）：二分类因子，水平为 <140 和>140",
    "总蛋白质（Proteins，β与α脂蛋白比率）：二分类因子，水平为<3和>3",
    "年家族史（Family，冠心病家族病史）：二分类因子，水平为阴性（neg）和阳性（pos）"
  )
)

format_kable_table(var_desc)
```

## 结构学习

### 约束优化算法 (PC算法)

```{r}
# PC算法学习网络结构
pc_net <- pc.stable(coronary) 
# 手动修改网络结构，确保为DAG
# pc_net <- drop.edge(pc_net, from = "pressure", to = "proteins")
pc_net <- set.arc(pc_net, from = "pressure", to = "proteins")
print("PC算法学习的网络结构:")
print(pc_net)
graphviz.plot(pc_net, main = "PC Algorithm Network")
```

### 评分搜索算法 (Hill Climbing)

```{r}
# Hill Climbing算法
hc_net <- hc(coronary)
print("Hill Climbing算法学习的网络结构:")
print(hc_net)

# 可视化
graphviz.plot(hc_net, main = "Hill Climbing Network")
```

### 混合算法 (MMHC)

```{r}
# MMHC算法 (Max-Min Hill Climbing)
mmhc_net <- mmhc(coronary)
print("MMHC算法学习的网络结构:")
print(mmhc_net)

# 可视化
graphviz.plot(mmhc_net, main = "MMHC Network")
```

## 使用交叉验证评估模型

```{r}
# 评估函数
evaluate_network<- function(net, data, k = 10) {
  n <- nrow(data)
  fold_size <- n %/% k
  scores <- numeric(k)
  successful_folds <- 0
  
  cat("评估网络:", paste(arcs(net), collapse = ", "), "\n")
  cat("网络边数:", nrow(arcs(net)), "\n")
  
  for (i in 1:k) {
    tryCatch({
      # 划分数据
      test_idx <- ((i-1) * fold_size + 1):(i * fold_size)
      if (i == k) test_idx <- ((i-1) * fold_size + 1):n
      
      train_data <- data[-test_idx, ]
      test_data <- data[test_idx, ]
      
      # 检查训练数据是否包含所有水平
      for (var in names(data)) {
        train_levels <- unique(train_data[[var]])
        test_levels <- unique(test_data[[var]])
        missing_levels <- setdiff(test_levels, train_levels)
        if (length(missing_levels) > 0) {
          cat("警告: 变量", var, "在训练集中缺少水平:", missing_levels, "\n")
        }
      }
      
      # 使用拉普拉斯平滑拟合参数
      fitted_net <- bn.fit(net, train_data, method = "bayes", iss = 1)
      
      # 计算对数似然
      ll <- logLik(fitted_net, test_data)
      scores[i] <- ll
      successful_folds <- successful_folds + 1
      
      cat("第", i, "折: ", round(ll, 2), "\n")
      
    }, error = function(e) {
      cat("第", i, "折出现错误:", e$message, "\n")
      scores[i] <- NA
    })
  }
  
  valid_scores <- scores[!is.na(scores)]
  
  return(list(
    mean_score = if (length(valid_scores) > 0) mean(valid_scores) else NA,
    std_score = if (length(valid_scores) > 0) sd(valid_scores) else NA,
    scores = scores,
    successful_folds = successful_folds
  ))
}

# 多种评估指标
comprehensive_evaluation <- function(net, data) {
  cat("\n=== 网络结构信息 ===\n")
  cat("节点数:", length(nodes(net)), "\n")
  cat("边数:", nrow(arcs(net)), "\n")
  cat("边:", toString(apply(arcs(net), 1, function(x) paste(x[1], "->", x[2]))), "\n")
  
  # BIC评分
  fitted_net <- bn.fit(net, data, method = "bayes", iss = 1)
  bic_score <- BIC(fitted_net, data)
  cat("BIC评分:", round(bic_score, 2), "\n")
  
  # AIC评分
  aic_score <- AIC(fitted_net, data)
  cat("AIC评分:", round(aic_score, 2), "\n")
  
  # 对数似然
  ll_score <- logLik(fitted_net, data)
  cat("对数似然:", round(ll_score, 2), "\n")
  
  # 交叉验证
  cv_result <- evaluate_network(net, data, k = 5)
  cat("交叉验证对数似然:", round(cv_result$mean_score, 2), "±", round(cv_result$std_score, 2), "\n")
  
  return(list(
    bic = bic_score,
    aic = aic_score,
    loglik = ll_score,
    cv_mean = cv_result$mean_score,
    cv_std = cv_result$std_score
  ))
}

# 评估所有网络
pc_comprehensive <- comprehensive_evaluation(pc_net, coronary)
hc_comprehensive <- comprehensive_evaluation(hc_net, coronary)
mmhc_comprehensive <- comprehensive_evaluation(mmhc_net, coronary)

# 结果汇总
results <- data.frame(
  Algorithm = c("PC", "Hill Climbing", "MMHC"),
  BIC = c(pc_comprehensive$bic, hc_comprehensive$bic, mmhc_comprehensive$bic),
  AIC = c(pc_comprehensive$aic, hc_comprehensive$aic, mmhc_comprehensive$aic),
  LogLikelihood = c(pc_comprehensive$loglik, hc_comprehensive$loglik, mmhc_comprehensive$loglik),
  CV_Mean = c(pc_comprehensive$cv_mean, hc_comprehensive$cv_mean, mmhc_comprehensive$cv_mean),
  CV_Std = c(pc_comprehensive$cv_std, hc_comprehensive$cv_std, mmhc_comprehensive$cv_std)
)

print(results)
```

## 参数学习

```{r}
# 选择最佳网络结构 (假设是mmHC算法的结果)
best_net <- mmhc_net

# 参数学习 - 使用贝叶斯估计（加入先验知识，避免零概率）
fitted_net <- bn.fit(best_net, coronary, method = "bayes", iss = 1)
# 拟合的贝叶斯网络
fitted_net

# 查看条件概率表
for(node in nodes(fitted_net)) {
  cat("\n节点", node, "的条件概率表:\n")
  print(fitted_net[[node]])
}

```

## 推理和预测

### 使用cpquery进行推理

```{r}
# gRain包进行精确推理

# 转换为gRain对象
grain_net <- as.grain(fitted_net)
grain_net <- compile(grain_net)

# P(Pressure | Smoking = "Yes")
evidence <- list(smoking = "yes")
grain_with_evidence <- setEvidence(grain_net, evidence = evidence)
pressure_given_smoking_yes <- querygrain(grain_with_evidence, nodes = "pressure")
print(pressure_given_smoking_yes)

# P(Smoking, Pressure | Proteins = "< 3")
evidence1 <- list(proteins = "< 3")
grain_with_evidence1 <- setEvidence(grain_net, evidence = evidence1)
joint_given_proteins <- querygrain(grain_with_evidence1, nodes = c("smoking", "pressure"))
cat("P(Smoking, Pressure | Proteins = < 3):\n")
print(joint_given_proteins)
```

## 模型诊断

### 独立性检验

```{r}
# 检验条件独立性假设
get_descendants <- function(net, node) {
  # 获取所有从该节点出发的后代
  descendants <- c()
  # 使用 bnlearn 的函数获取子节点
  children_nodes <- bnlearn::children(net, node)
  
  if(length(children_nodes) > 0) {
    descendants <- children_nodes
    for(child in children_nodes) {
      descendants <- c(descendants, get_descendants(net, child))
    }
  }
  
  return(unique(descendants))
}

# 条件独立性检验

for(node in bnlearn::nodes(best_net)) {
  parents_node <- bnlearn::parents(best_net, node)
  
  if(length(parents_node) > 0) {
    # 检验节点与其非后代的条件独立性
    descendants_node <- get_descendants(best_net, node)
    non_descendants <- setdiff(bnlearn::nodes(best_net), 
                              c(node, descendants_node, parents_node))
    
    cat("\n节点:", node, "的条件独立性检验:\n")
    cat("父节点:", paste(parents_node, collapse = ", "), "\n")
    cat("后代节点:", paste(descendants_node, collapse = ", "), "\n")
    
    for(other_node in non_descendants) {
      tryCatch({
        ci_test <- bnlearn::ci.test(node, other_node, parents_node, 
                          test = "x2", data = coronary)
        cat(node, "⊥", other_node, "|", paste(parents_node, collapse = ","), 
            "p-value:", round(ci_test$p.value, 4))
        if(ci_test$p.value > 0.05) {
          cat(" ✓ 独立\n")
        } else {
          cat(" ✗ 不独立\n")
        }
      }, error = function(e) {
        cat(node, "⊥", other_node, "|", paste(parents_node, collapse = ","), 
            "检验失败:", e$message, "\n")
      })
    }
  }
}

```

### 敏感性分析

```{r}
# 网络结构的稳定性分析
set.seed(123)
bootstrap_nets <- boot.strength(coronary, R = 100, algorithm = "hc")

# 查看边的置信度
print("边的置信度 (Bootstrap支持度):")
print(bootstrap_nets[bootstrap_nets$strength > 0.5, ])

# 平均网络
avg_net <- averaged.network(bootstrap_nets, threshold = 0.5)
graphviz.plot(avg_net, main = "Averaged Network (threshold = 0.5)")
```

贝叶斯网络为理解复杂变量间的依赖关系提供了强大的工具，特别适用于不确定性推理和决策支持系统。通过coronary数据集的分析，我们可以看到贝叶斯网络在医疗风险评估中的实际应用价值。

